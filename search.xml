<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[二分初解]]></title>
    <url>%2F2019%2F08%2F24%2F%E4%BA%8C%E5%88%86%E5%88%9D%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[适用范围从已经排好顺序的数组当中找到目标所在位置或者是正确地向数组中插入一个目标。 思想将整个数组不停地拆分为两段（每一段有左边界left和右边界right），用分隔两段的数（mid处值）与目标相比较，观察目标处于哪一段，最终段长小于2时，便可唯一确定位置，其时间复杂度为log2n。 划分的具体过程如下列数组{1,2,4,8,16},target=11，查找其应该插入的位置，left意味着target至少应该从该位置插入 步骤 说明 a[0] a[1] a[2] a[3] a[4] target 1 2 4 8 16 11 0 left=0,right=4,mid=(left+right)/2=2 left mid right 1 因a[mid]=4&lt;targetleft需右移至mid+1故left=3,right=4,mid = (3+4)/2 = 3 leftmid right 2 a[mid]=8&lt;target,left右移至4mid = 4 leftmidright 3 到这一步段距为1了，再做一下判断target&lt;a[mid]=16,因此right=mid-1=3,至此，左边界超过了右边界，那么左边界就是我们需要查找的位置 right leftmid 存在问题中值计算mid = (left+right)/2在left和right值很大的时候可能造成数值溢出导致结果错误，因此可以换一种写法mid = left+(right-left)/2。 经过了解，还有另外一种写法就是mid = (left+right)&gt;&gt;&gt;1,其中，&gt;&gt;&gt;表示无符号右移运算符，其在右移时左边空位会补上0，也就是会变成正数。 确定边界在实际实现时，确定左右边界是需要考虑好几种情况的，如上表是将数组的下标直接用作左右边界。]]></content>
      <tags>
        <tag>算法</tag>
        <tag>基础算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Android实践时的问题]]></title>
    <url>%2F2019%2F05%2F18%2FAndroid%E5%AE%9E%E8%B7%B5%E6%97%B6%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[踩坑记； WebViewWebViewClient 1, 若没有设置 WebViewClient 则在点击链接之后由系统处理该 url，通常是使用浏览器打开或弹出浏览器选择对话框。2, 若设置 WebViewClient 且该方法返回 true ，则说明由应用的代码处理该 url，WebView 不处理。3, 若设置 WebViewClient 且该方法返回 false，则说明由 WebView处理该 url，即用 WebView 加载该 url。 在网上搜索出来的在webview内打开url的方法如下：12345678webview.setWebViewClient(new WebViewClient()&#123; @Override public boolean shouldOverrideUrlLoading(WebView view,String url)&#123; view.loadUrl(url); return false; &#125; &#125; &#125;); 可看出该方法直接重写WebViewClient，return false使情况3达成，我一开始就这样直接照搬了，然后一下午我都一直在遇到net::ERR_UNKNOWN_URL_SCHEME这个错误，发生在某度搜索结果的各种跳转界面。是夜，得到解决办法：12345678910111213webview.setWebViewClient(new WebViewClient()&#123; @Override public boolean shouldOverrideUrlLoading(WebView view,String url)&#123; if(url.startsWith("http:") || url.startsWith("https:") ) &#123; view.loadUrl(url); return false; &#125;else&#123; Intent intent2 = new Intent(Intent.ACTION_VIEW, Uri.parse(url)); startActivity(intent2); return true; &#125; &#125; &#125;); 增加了一个判断，如果是非http(s)型，如百度微信电话邮件等跳转链接，则将其传递到能响应此链接的应用中，返回true，对应类型2。 我天真地以为这就是一切了，然后点击打不开的跳转页面，不报net错误了，第一次直接闪回主界面，第二次就整个程序崩溃了。苦思，无果，遂寻一佬名fish，佬言待其catch一异常，然尚未及现log，页面竟已正常跳转，大惊，忙询佬其缘。12345678910111213141516171819webview.setWebViewClient(new WebViewClient()&#123; @Override public boolean shouldOverrideUrlLoading(WebView view,String url)&#123; if(url.startsWith("http:") || url.startsWith("https:") ) &#123; view.loadUrl(url); return false; &#125;else&#123; try&#123; Intent intent2 = new Intent(Intent.ACTION_VIEW, Uri.parse(url)); startActivity(intent2); return true; &#125; catch (Exception ex)&#123; Log.d("","TEST TEST:" + ex.getMessage()); return true; &#125; &#125; &#125; &#125;); 佬曰：如果存在可以打开链接的app，webview就会通过intent申请打开该应用，如果不存在，就会先报错不存在该应用然后提示下载该应用，我这代码没有处理报错，于是程序就直接卡在这一部崩溃而不进行下一步的下载界面跳转。佬还曰：写函数必先catch e，对防止程序崩溃是极好的。事毕，愧，记以纪之。 因为遇见了返回重定向的问题，又仔细检查了一遍，发现123if(url.startsWith("http:") || url.startsWith("https:") ) &#123; view.loadUrl(url); return false;&#125; 这里应该直接return false就好了。123456789101112131415161718webview.setWebViewClient(new WebViewClient()&#123; @Override public boolean shouldOverrideUrlLoading(WebView view,String url)&#123; if(url.startsWith("http:") || url.startsWith("https:") ) &#123; return false; &#125;else&#123; try&#123; Intent intent2 = new Intent(Intent.ACTION_VIEW, Uri.parse(url)); startActivity(intent2); return true; &#125; catch (Exception ex)&#123; Log.d("","TEST TEST:" + ex.getMessage()); return true; &#125; &#125; &#125; &#125;); 应该是最终版了，希望是最终版了。把匹配的判定改了一下1234567891011121314151617webView.setWebViewClient(new WebViewClient() &#123; @Override public boolean shouldOverrideUrlLoading(WebView view, final String url) &#123; if (("https".equalsIgnoreCase(Uri.parse(url).getScheme())) || ("http".equalsIgnoreCase(Uri.parse(url).getScheme()))) &#123; return false; &#125; else &#123; try &#123; Intent intent = new Intent(Intent.ACTION_VIEW, Uri.parse(url)); startActivity(intent); return true; &#125; catch (Exception ex) &#123; return true; &#125; &#125; &#125; &#125;); WebSettings在bilibili手机版网页点击视频后，会在一两秒内白屏，上网寻之，webview中屏蔽bilibili启动客户端app的请求，有同样的问题，然而经测试并没有什么用，查看控制台error，才发现”Uncaught TypeError: Cannot read property ‘getItem’ of null”这个错误提示，经过搜索： 是因为该网页的js写得不规范(擦，原来是前端哥们挖的坑)：在JS运行的时候你的页面还没有加载完成，所以你的JS代码找不到你的页面元素，就会抛出这个问题。在网页端修改方法：把JS的引入文件，放在文档后面就可以避免了。当然，在安卓端修改也只是一句话的事： webView.getSettings().setDomStorageEnabled(true);就可以忽略这个问题执行后面的方法This seems to allow the browser to store a DOM model of the page elements, so that Javascript can perform operations on it.此处就允许浏览器保存doom原型，这样js就可以调用这个方法了‘).addClass(‘pre-numbering’).hide(); $(this).addClass(‘has-numbering’).parent().append($numbering); for (i = 1; i &lt;= lines; i++) { $numbering.append($(‘‘).text(i)); }; $numbering.fadeIn(1700); }); }); 无法打开网易云音乐，QQ音乐的播放页面，终端错误为在一http音频链接后This request has been blocked; the content must be served over HTTPS.结果： 情况一：无法加载JS内容写的页面。设置setDomStorageEnabled，可能是webview前端写的一些代码便签不支持。WebSettings settings = webview.getSettings();settings.setJavaScriptEnabled(true);settings.setDomStorageEnabled(true);情况二：出现部分内容显示部分内容不显示，打印日志大概如下：chromium: [INFO:CONSOLE(15)] “Mixed Content: The page at ‘https://dihao.moxz.cn/&#39; was loaded over HTTPS, but requested an insecure image ‘http://sm.domobcdn.com/hm/2017/landing/img/dihao/p5.jpg&#39;. This request has been blocked; the content must be served over HTTPS.”, source: https://sm.domobcdn.com/hm/2017/landing/js/swiper.3.4.2.min.js (15)这个是加载的地址是https的，一些资源文件使用的是http方法的，从安卓4.4之后对webview安全机制有了加强，webview里面加载https url的时候，如果里面需要加载http的资源或者重定向的时候，webview会block页面加载。需要设置MixedContentMode，解决此问题的代码如下： WebSettings settings = webView.getSettings(); settings.setJavaScriptEnabled(true); settings.setDomStorageEnabled(true); settings.setAllowFileAccess(true); settings.setAppCacheEnabled(true); if (Build.VERSION.SDK_INT &gt;= Build.VERSION_CODES.LOLLIPOP) { webView.getSettings().setMixedContentMode(WebSettings.MIXED_CONTENT_ALWAYS_ALLOW); }]]></content>
      <tags>
        <tag>Android</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单的中缀表达式转后缀及计算]]></title>
    <url>%2F2019%2F05%2F17%2F%E7%AE%80%E5%8D%95%E7%9A%84%E4%B8%AD%E7%BC%80%E8%A1%A8%E8%BE%BE%E5%BC%8F%E8%BD%AC%E5%90%8E%E7%BC%80%E5%8F%8A%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[参考着做了一下，没有检查输入是否正确，其他应该都差不多了，为了存下浮点数，因此用了string数组，导致增加了一些类型转换函数，最后的结果也只有五位小数了，暂时懒得去想了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;stack&gt;#include &lt;vector&gt;using namespace std;int pri(char x)&#123; if(x=='/' || x=='*') return 2; if(x=='+' || x=='-') return 1; if(x=='(') return 0; else return -1;&#125;vector&lt;string&gt; in_to_post(string input)&#123; stack&lt;char&gt; s; vector&lt;string&gt; post; string t=""; int i=0; while(input[i]) &#123; char tmp = (char)input[i]; if(tmp &gt;= '0' &amp;&amp; tmp &lt;= '9') &#123; string num_tmp; int j = i; while((tmp &gt;= '0' &amp;&amp; tmp &lt;= '9') || tmp == '.') &#123; num_tmp += tmp; tmp = (char)input[++j]; &#125; post.push_back(num_tmp); i = j-1; &#125; else &#123; if(s.empty()) &#123; s.push(tmp); &#125; else if(tmp=='(') &#123; s.push(tmp); &#125; else if(tmp==')') &#123; while(s.top()!='(') &#123; t = ""; t += s.top(); post.push_back(t); s.pop(); &#125; s.pop(); &#125; else &#123; while(pri(tmp)&lt;=pri(s.top())) &#123; t = ""; t += s.top(); post.push_back(t); s.pop(); if(s.empty()) break; &#125; s.push(tmp); &#125; &#125; i++; &#125; if(!s.empty()) &#123; t = ""; t += s.top(); post.push_back(t); s.pop(); &#125; return post;&#125;double cal(double a,double b,char c)&#123; if(c=='+') return a+b; else if(c=='-') return a-b; else if(c=='*') return a*b; else if(c=='/') return a/b; else return 0;&#125;int myisdigit(string s)&#123; int i=0; while(i&lt;s.size()) &#123; if(s[i]!='.'&amp;&amp;(s[i]&lt;'0'||s[i]&gt;'9')) return 0; i++; &#125; return 1;&#125;double postcal(vector&lt;string&gt; in)&#123; stack&lt;string&gt; s; int i=0; while(i&lt;in.size()) &#123; string tmp = in[i]; if(myisdigit(tmp)) &#123; s.push(tmp); &#125; else &#123; double b = stod(s.top()); s.pop(); double a= stod(s.top()); s.pop(); char c = tmp[0]; double r = cal(a,b,c); s.push(to_string(r)); &#125; i++; &#125; return stod(s.top());&#125;int main()&#123; string input; int i=0; vector&lt;string&gt; postinput; cout&lt;&lt;"请输入中缀表达式："&lt;&lt;endl; cin&gt;&gt;input; postinput=in_to_post(input); cout&lt;&lt;"后缀表达式为："&lt;&lt;endl; for(i=0; i&lt;postinput.size(); i++) &#123; cout&lt;&lt;postinput[i]&lt;&lt;" "; &#125; double res = postcal(postinput); cout&lt;&lt;endl&lt;&lt;"结果为："&lt;&lt;res&lt;&lt;endl; return 0;&#125;/*请输入中缀表达式：1.2*((2/4.3)*3.2)后缀表达式为：1.2 2 4.3 / 3.2 * *结果为：1.78605to_string stod均为c++11特性,需编译器支持,in codeblocks,go to settings-&gt;compiler-&gt;check the "have g++ follow the c++11 ISO..."*/]]></content>
      <tags>
        <tag>逆波兰</tag>
        <tag>栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[编译原理]]></title>
    <url>%2F2019%2F04%2F02%2F%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[第一章 引论什么叫编译程序翻译程序：把某一种语言程序（称为源语言程序）转换成另一种语言程序（称为目标语言程序）。 解释程序：以源语言写的源程序作为输入，但不产生目标程序，而是边解释边执行源程序本身。优点：易于查错缺点：效率低，运行速度慢 编译程序：源语言为高级语言，目标语言为汇编语言或机器语言之类的“低级语言”的翻译程序。优点：只需分析和翻译一次，缺点：在运行中发现的错误必须在源程序中查找 运行编译程序的计算机称宿主机，运行编译程序所产生目标代码的计算机称目标机 诊断编译程序：专门用于帮助程序开发和调试 优化编译程序：着重于提高目标代码效率 交叉编译程序：产生不同于其宿主机的机器代码 可变目标编译程序：不需重写编译程序中与机器无关的部分就能改变目标机 世界上第一个编译程序–FORTRAN编译程序是20世纪50年代中期研制成功的 编译程序的结构词法分析器：又称扫描器，输入源程序，进行词法分析，输出单词符号。 语法分析器：简称分析器，对单词符号串进行语法分析（根据与法规则进行推导或归约），识别出各类语法单位，最终判断输入串是否构成语法上正确的“程序”。 语义分析与中间代码产生器：按照语义规则对语法分析器归约出（或推导出）的语法单位进行语义分析并把它们翻译成一定形式的中间代码。中间代码可采用四元式、三元式、间接三元式、逆波兰记号和树形表示等等。例如：许多编译程序采用四元序列形式：Z := (X + 0.418)*Y/w 翻译成四元式为： 优化器：对中间代码进行优化处理。 目标代码生成器：把中间代码翻译成目标程序。 表格中最重要的是符号表，出错处理中出错处理程序。 遍：具体实现编译过程时，受不同源语言、设计要求、使用对象和计算机条件（如主存容量）的限制，往往将编译程序组织为若干遍（Pass）。所谓“遍”就是对源程序或源程序的中间结果从头到尾扫描一次，并作有关的加工处理，生成新的中间结果或目标程序。 编译前端与后端：前端主要由与源语言有关但与目标机无关的那些部分（词法分析、语法分析、语义分析、中间代码产生）组成，后端反之，包括与目标机有关的代码优化和目标代码生成等。通常，后端不依赖于源语言而仅仅依赖于中间语言。 Ada程序设计环境APSE中使用一种称为Diana的树形结构的中间语言；Java语言环境中，虚拟机代码Bytecode。 程序设计环境：编译程序与编辑程序、连接程序、调试工具等一起构成。 编译程序的生成]]></content>
      <tags>
        <tag>编译原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬取微博图片]]></title>
    <url>%2F2019%2F03%2F20%2Fpython%E7%88%AC%E5%8F%96%E5%BE%AE%E5%8D%9A%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[代码来自 作者：cool_flag来源：CSDN原文：https://blog.csdn.net/cool_flag/article/details/78992628 本来以为微博应该会跟百度贴吧一样简单，开始动手后才发现不是我想的那样，在百度搜索后找到了上面这篇帖子，受益匪浅。该作者使用的是微博移动版https://weibo.cn其原话是 因为html5版的微博页面和web端一样也是采用动态加载的，什么是动态加载呢，就是你打开一页可以一直往下翻，他会一直帮你自动加载，直到底部。这就导致了代码是根据你的行为(鼠标的点击等等)不断变化的。 接着第二个问题是模拟登陆，开始我想着是post模拟登陆输入用户账号和密码即可，后来发现是我想多了…好像有点困难，于是就采用了cookie直接登陆，这个方法简单粗暴。 获取cookie方法1 点击进入移动版微博https://weibo.cn2 登陆3 进入想要爬取图片的博主的主页，如https://weibo.cn/u/18563345354 谷歌浏览器F12打开调试窗口5 刷新，点击第一个文件6 向下寻找Request Headers，其中的cookie项一长串即为所需，复制下来 遇到的问题原作者有一段获取微博发送时间的代码123#获取微博发送时间hms = ' '.join(weibo_tag.find( 'span', attrs=&#123;'class': 'ct'&#125;).text.replace('\xa0', '').replace(':', '-').split(' ')[:2]) 我一开始运行到这里就会报错，大致内容是找不到，然后经过排查是因为截取微博代码段的代码weibo_tags_list = soup.find_all(&#39;div&#39;, attrs={&#39;class&#39;: &#39;c&#39;}, id=True)有没考虑完全的情况，如微博https://weibo.cn/u/1856334535其源代码经过查找后会有下面两种：123456789101112&lt;div class="c" id="M_HlmjxqNMs"&gt; &lt;div&gt; &lt;span class="ctt"&gt;GRL&lt;br/&gt;斋藤飞鸟X西野七濑 &lt;/span&gt; &lt;/div&gt; &lt;div&gt; &lt;a href="https://weibo.cn/mblog/pic/HlmjxqNMs?rl=0"&gt; &lt;img alt="图片" class="ib" src="http://wx2.sinaimg.cn/wap180/6ea56ac7ly1g15zu9yuunj20u011igqp.jpg"/&gt; &lt;/a&gt; &lt;a href="https://weibo.cn/mblog/oripic?id=HlmjxqNMs&amp;amp;u=6ea56ac7ly1g15zu9yuunj20u011igqp"&gt;原图&lt;/a&gt; &lt;br/&gt; &lt;a href="https://weibo.cn/attitude/HlmjxqNMs/add?uid=6070849388&amp;amp;rl=0&amp;amp;st=3622c9"&gt;赞[786]&lt;/a&gt; &lt;a href="https://weibo.cn/repost/HlmjxqNMs?uid=1856334535&amp;amp;rl=0"&gt;转发[67]&lt;/a&gt; &lt;a class="cc" href="https://weibo.cn/comment/HlmjxqNMs?uid=1856334535&amp;amp;rl=0#cmtfrm"&gt;评论[30]&lt;/a&gt; &lt;a href="https://weibo.cn/fav/addFav/HlmjxqNMs?rl=0&amp;amp;st=3622c9"&gt;收藏&lt;/a&gt; &lt;!-- --&gt; &lt;span class="ct"&gt;03月17日 19:00 来自微博 weibo.com&lt;/span&gt; &lt;/div&gt;&lt;/div&gt; 1234567&lt;div class="c" id="M_HldFo0Vu8"&gt; &lt;div&gt; &lt;span class="ctt"&gt; &lt;img alt="[haha]" src="//h5.sinaimg.cn/m/emoticon/icon/others/h_haha-6934824adc.png" style="width:1em; height:1em;"/&gt; &lt;/span&gt; &lt;/div&gt;&lt;/div&gt; 一开始我的想法是对find_all的条件增加限制，比如在原来的div标签中查找到img alt=&quot;图片&quot; class=&quot;ib&quot;，然而上网搜索了一段时间后还是不知道应该怎么写，最后我放弃了这种想法，转而思考在没有获取到正确的代码段时不获取时间。于是便有了下面的一段代码12345if weibo_tag.find('span', text=re.compile('来自微博')): hms = ' '.join(weibo_tag.find( 'span', attrs=&#123;'class': 'ct'&#125;).text.replace('\xa0', ' ').replace(':', '-').split(' ')[:2])else: hms = "none"+"-"+str(page)+"-"+str(entry) 法子是笨了点，结果也导致最后文件夹里多了很多none-xx-xx的图片，但好歹是运行起来了。 思考在第一次爬取贴吧图片时网页获取我是用的urllib.request.urlopen(url)，结果运行时会出现错误urllib.error.URLError: &lt;urlopen error [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。这次的代码里面使用的是requests.get(url, headers=headers)，然后一直运行也没有什么错误，不知道是因为不同方式还是因为这次请求连接使用了cookie的原因，今天时间不多了，便暂不思考，留待以后吧。 附 最终代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182import requestsfrom bs4 import BeautifulSoup as bsimport osimport re#uid即进入对方微博主页后网址部分/u/后的那一串数字uid = input('请输入所要爬取的用户id:')url = 'https://weibo.cn/u/'+uidcookie = input('请输入你的新浪微博的cookie:')headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.108 Safari/537.36', 'Cookie': cookie&#125;r = requests.get(url, headers=headers)soup = bs(r.text, 'html.parser')# 所访问的微博用户名weibo_user_name = soup.find('title').text.replace('的微博', '')# 存放图片的根目录print(weibo_user_name)rootDir = 'E://pic//' + weibo_user_name + '//'# 微博总页数,通过审查元素可知totalPage = int(soup.find('input', attrs=&#123;'name': 'mp'&#125;).attrs['value'])print('总共检测到%d页微博页面' % totalPage)# 每页微博的URL的列表weibo_urlList = [url + '?page=' + str(i + 1) for i in range(totalPage)]#当前已爬取的图片总数pictrue_num = 0for page, weibo_url in enumerate(weibo_urlList): r = requests.get(weibo_url, headers=headers) soup = bs(r.text, 'html.parser') #每条微博所对应的标签代码块列表 weibo_tags_list = soup.find_all('div', attrs=&#123;'class': 'c'&#125;, id=True) #微博发送时间与微博配图的字典 imgs_urls_dic = &#123;&#125; for entry, weibo_tag in enumerate(weibo_tags_list): print('正在爬取第%d页第%d条微博' % (page + 1, entry + 1)) #获取微博发送时间 if weibo_tag.find('span', text=re.compile('来自微博')): hms = ' '.join(weibo_tag.find( 'span', attrs=&#123;'class': 'ct'&#125;).text.replace('\xa0', ' ').replace(':', '-').split(' ')[:2]) else: hms = "none"+"-"+str(page)+"-"+str(entry) #该条微博若带有组图，获取组图中所有图片的URL if weibo_tag.find('a', text=re.compile('组图')): imgs_url = weibo_tag.find('a', text=re.compile('组图')).attrs['href'] html = requests.get(imgs_url, headers=headers) imgs_soup = bs(html.text, 'html.parser') imgs_tags_List = imgs_soup.find_all('img', alt='图片加载中...') img_urls_list = [imgs_tag.attrs['src'].replace( 'thumb180', 'large') for imgs_tag in imgs_tags_List] imgs_urls_dic[hms] = img_urls_list #该条微博仅有一张配图，或者没有图片，获取图片的URL else: img_tags_List = weibo_tag.find_all('img', alt='图片') img_urls_list = [img_tag.attrs['src'].replace( 'wap180', 'large') for img_tag in img_tags_List] imgs_urls_dic[hms] = img_urls_list print('第%d页微博爬取完毕,开始生成图片' % (page + 1)) for hms, img_urls_list in imgs_urls_dic.items(): for index, img_url in enumerate(img_urls_list): #生成图片的存放路径，图片被命名为微博发送时间 path = rootDir + hms #如果一条微博在同一时间发送了多张图片(即组图)的命名处理 if(index &gt; 0): path = path + '(' + str(index) + ')' path = path + '.jpg' try: if not os.path.exists(rootDir): #makedirs递归生成多级目录，mkdir仅能生成一级目录 os.makedirs(rootDir) if not os.path.exists(path): r = requests.get(img_url) with open(path, 'wb') as f: f.write(r.content) pictrue_num = pictrue_num + 1 print('success,成功爬取第%d张图片' % pictrue_num) else: print('%s已经存在' % path) except: print('爬取失败,%s' % img_url)print('总共爬取了%d张图片，存放在 %s 目录下' % (pictrue_num, rootDir))]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[纪念第一次用python]]></title>
    <url>%2F2019%2F03%2F18%2F%E7%BA%AA%E5%BF%B5%E7%AC%AC%E4%B8%80%E6%AC%A1%E7%94%A8python%2F</url>
    <content type="text"><![CDATA[用python3爬取百度贴吧图片1 寻找想要爬取图片的网页，如http://tieba.baidu.com/p/60273185722 查看网页源代码 3 放代码1234567891011121314151617181920212223242526272829303132333435363738# coding:utf-8import urllib.requestimport reimport osdef get_html(url): page = urllib.request.urlopen(url) html = page.read() return htmld = os.getcwd()p = "\\pic"isExists = os.path.exists(d+p)if not isExists: os.mkdir(d+p) print("创建文件夹成功")else: print("文件夹已存在") imgName = 0for i in range(3): html = get_html("http://tieba.baidu.com/p/6027318572?see_lz=1&amp;pn="+str(i+1)) html = html.decode('utf-8') reg = r'img class="BDE_Image" src="(.+?\.jpg)" size' reg_img = re.compile(reg) imglist = re.findall(reg_img, html) for imgPath in imglist: print(imgPath) f = open("pic/"+str(imgName)+".jpg", 'wb') f.write((urllib.request.urlopen(imgPath)).read()) f.close() imgName += 1print(str(imgName)+" img have been saved in pic")print("All Done!") 4 最终效果 附 半自动实现爬取图片，需要手输入链接及页数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# coding:utf-8import urllib.requestimport reimport timedef mkdir(): import os path = os.getcwd() LocalPath = input("Enter folder name to save pics(in now path): "); path = path+"\\"+LocalPath isExists=os.path.exists(path) if not isExists: # 如果不存在则创建目录 # 创建目录操作函数 os.makedirs(path) print (path+" 创建成功") else: # 如果目录存在则不创建，并提示目录已存在 print (path+" 已存在") return LocalPathLocalPath = mkdir()def get_html(url): page = urllib.request.urlopen(url) html = page.read() return htmldef get_url(): url = input("输入想要获取源码的网页链接: "); url = url+'?see_lz=' OrLz = input("只看楼主？(0否/1是): "); OrLz = str(OrLz) url = url+OrLz print(url) return urlurlFirst = get_url()page = input("输入页数:");page = int(page)imgName = 0for i in range(page): url = urlFirst+'&amp;pn='+str(i+1) print(url) html = get_html(url) html = html.decode('utf-8') reg = r'img class="BDE_Image" src="(.+?\.jpg)"' reg_img = re.compile(reg) imglist = re.findall(reg_img, html) for imgPath in imglist: #print(imgPath) f = open(LocalPath+"/"+str(imgName)+".jpg", 'wb') f.write((urllib.request.urlopen(imgPath)).read()) f.close() imgName += 1 if(imgName%10 == 0): print(str(imgName)+" imgs have been saved in "+LocalPath) time.sleep(5)print("All "+str(imgName)+" imgs have been saved in "+LocalPath) 问题: urllib.error.URLError: &lt;urlopen error [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。]]></content>
      <tags>
        <tag>Saito Asuka</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo上传图片]]></title>
    <url>%2F2019%2F03%2F10%2Fhexo%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[如何上传本地图片1 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true2 在hexo目录下执行 npm install hexo-asset-image –save安装一个可以上传本地图片的插件3 等待一小段时间后，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还会生成一个同名的文件夹4 最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片：![你想输入的替代文字](xxxx/图片名.jpg) 如何上传在线图片1 选择图床，如https://sm.ms/2 上传图片后就能直接生成带有链接的图片 代码为1[![鸟2.jpg](https://i.loli.net/2019/03/18/5c8f7ed81bffd.jpg)](https://i.loli.net/2019/03/18/5c8f7ed81bffd.jpg)]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>Saito Asuka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 使用]]></title>
    <url>%2F2019%2F03%2F10%2FHexo-%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[hexo new “postName” #新建文章hexo new page “pageName” #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，’ctrl + c’关闭server）hexo deploy #部署到GitHubhexo help # 查看帮助hexo version #查看Hexo的版本 删除文章直接本地删除md文件，然后执行hexo clean 使用时只需如 hexo g 一样写出首字母就行 hexo s -g #生成并本地预览hexo d -g #生成并上传 设置文章摘要的长度:在合适的位置加上&lt;!--more--&gt;即可]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Saito Asuka]]></title>
    <url>%2F2019%2F03%2F10%2FSaito-Asuka%2F</url>
    <content type="text"><![CDATA[齋藤飛鳥だいすき]]></content>
      <tags>
        <tag>Saito Asuka</tag>
      </tags>
  </entry>
</search>
