<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[编译原理]]></title>
    <url>%2F2019%2F04%2F02%2F%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[第一章 引论什么叫编译程序翻译程序：把某一种语言程序（称为源语言程序）转换成另一种语言程序（称为目标语言程序）。解释程序：以源语言写的源程序作为输入，但不产生目标程序，而是边解释边执行源程序本身。编译程序：源语言为高级语言，目标语言为汇编语言或机器语言之类的“低级语言”的翻译程序。运行编译程序的计算机称宿主机，运行编译程序所产生目标代码的计算机称目标机 诊断编译程序：专门用于帮助程序开发和调试 优化编译程序：着重于提高目标代码效率 交叉编译程序：产生不同于其宿主机的机器代码 可变目标编译程序：不需重写编译程序中与机器无关的部分就能改变目标机 世界上第一个编译程序–FORTRAN编译程序是20世纪50年代中期研制成功的 编译程序的结构]]></content>
      <tags>
        <tag>编译原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬取微博图片]]></title>
    <url>%2F2019%2F03%2F20%2Fpython%E7%88%AC%E5%8F%96%E5%BE%AE%E5%8D%9A%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[代码来自 作者：cool_flag来源：CSDN原文：https://blog.csdn.net/cool_flag/article/details/78992628 本来以为微博应该会跟百度贴吧一样简单，开始动手后才发现不是我想的那样，在百度搜索后找到了上面这篇帖子，受益匪浅。该作者使用的是微博移动版https://weibo.cn其原话是 因为html5版的微博页面和web端一样也是采用动态加载的，什么是动态加载呢，就是你打开一页可以一直往下翻，他会一直帮你自动加载，直到底部。这就导致了代码是根据你的行为(鼠标的点击等等)不断变化的。 接着第二个问题是模拟登陆，开始我想着是post模拟登陆输入用户账号和密码即可，后来发现是我想多了…好像有点困难，于是就采用了cookie直接登陆，这个方法简单粗暴。 获取cookie方法1 点击进入移动版微博https://weibo.cn2 登陆3 进入想要爬取图片的博主的主页，如https://weibo.cn/u/18563345354 谷歌浏览器F12打开调试窗口5 刷新，点击第一个文件6 向下寻找Request Headers，其中的cookie项一长串即为所需，复制下来 遇到的问题原作者有一段获取微博发送时间的代码123#获取微博发送时间hms = ' '.join(weibo_tag.find( 'span', attrs=&#123;'class': 'ct'&#125;).text.replace('\xa0', '').replace(':', '-').split(' ')[:2]) 我一开始运行到这里就会报错，大致内容是找不到，然后经过排查是因为截取微博代码段的代码weibo_tags_list = soup.find_all(&#39;div&#39;, attrs={&#39;class&#39;: &#39;c&#39;}, id=True)有没考虑完全的情况，如微博https://weibo.cn/u/1856334535其源代码经过查找后会有下面两种：123456789101112&lt;div class="c" id="M_HlmjxqNMs"&gt; &lt;div&gt; &lt;span class="ctt"&gt;GRL&lt;br/&gt;斋藤飞鸟X西野七濑 &lt;/span&gt; &lt;/div&gt; &lt;div&gt; &lt;a href="https://weibo.cn/mblog/pic/HlmjxqNMs?rl=0"&gt; &lt;img alt="图片" class="ib" src="http://wx2.sinaimg.cn/wap180/6ea56ac7ly1g15zu9yuunj20u011igqp.jpg"/&gt; &lt;/a&gt; &lt;a href="https://weibo.cn/mblog/oripic?id=HlmjxqNMs&amp;amp;u=6ea56ac7ly1g15zu9yuunj20u011igqp"&gt;原图&lt;/a&gt; &lt;br/&gt; &lt;a href="https://weibo.cn/attitude/HlmjxqNMs/add?uid=6070849388&amp;amp;rl=0&amp;amp;st=3622c9"&gt;赞[786]&lt;/a&gt; &lt;a href="https://weibo.cn/repost/HlmjxqNMs?uid=1856334535&amp;amp;rl=0"&gt;转发[67]&lt;/a&gt; &lt;a class="cc" href="https://weibo.cn/comment/HlmjxqNMs?uid=1856334535&amp;amp;rl=0#cmtfrm"&gt;评论[30]&lt;/a&gt; &lt;a href="https://weibo.cn/fav/addFav/HlmjxqNMs?rl=0&amp;amp;st=3622c9"&gt;收藏&lt;/a&gt; &lt;!-- --&gt; &lt;span class="ct"&gt;03月17日 19:00 来自微博 weibo.com&lt;/span&gt; &lt;/div&gt;&lt;/div&gt; 1234567&lt;div class="c" id="M_HldFo0Vu8"&gt; &lt;div&gt; &lt;span class="ctt"&gt; &lt;img alt="[haha]" src="//h5.sinaimg.cn/m/emoticon/icon/others/h_haha-6934824adc.png" style="width:1em; height:1em;"/&gt; &lt;/span&gt; &lt;/div&gt;&lt;/div&gt; 一开始我的想法是对find_all的条件增加限制，比如在原来的div标签中查找到img alt=&quot;图片&quot; class=&quot;ib&quot;，然而上网搜索了一段时间后还是不知道应该怎么写，最后我放弃了这种想法，转而思考在没有获取到正确的代码段时不获取时间。于是便有了下面的一段代码12345if weibo_tag.find('span', text=re.compile('来自微博')): hms = ' '.join(weibo_tag.find( 'span', attrs=&#123;'class': 'ct'&#125;).text.replace('\xa0', ' ').replace(':', '-').split(' ')[:2])else: hms = "none"+"-"+str(page)+"-"+str(entry) 法子是笨了点，结果也导致最后文件夹里多了很多none-xx-xx的图片，但好歹是运行起来了。 思考在第一次爬取贴吧图片时网页获取我是用的urllib.request.urlopen(url)，结果运行时会出现错误urllib.error.URLError: &lt;urlopen error [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。这次的代码里面使用的是requests.get(url, headers=headers)，然后一直运行也没有什么错误，不知道是因为不同方式还是因为这次请求连接使用了cookie的原因，今天时间不多了，便暂不思考，留待以后吧。 附 最终代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182import requestsfrom bs4 import BeautifulSoup as bsimport osimport re#uid即进入对方微博主页后网址部分/u/后的那一串数字uid = input('请输入所要爬取的用户id:')url = 'https://weibo.cn/u/'+uidcookie = input('请输入你的新浪微博的cookie:')headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.108 Safari/537.36', 'Cookie': cookie&#125;r = requests.get(url, headers=headers)soup = bs(r.text, 'html.parser')# 所访问的微博用户名weibo_user_name = soup.find('title').text.replace('的微博', '')# 存放图片的根目录print(weibo_user_name)rootDir = 'E://pic//' + weibo_user_name + '//'# 微博总页数,通过审查元素可知totalPage = int(soup.find('input', attrs=&#123;'name': 'mp'&#125;).attrs['value'])print('总共检测到%d页微博页面' % totalPage)# 每页微博的URL的列表weibo_urlList = [url + '?page=' + str(i + 1) for i in range(totalPage)]#当前已爬取的图片总数pictrue_num = 0for page, weibo_url in enumerate(weibo_urlList): r = requests.get(weibo_url, headers=headers) soup = bs(r.text, 'html.parser') #每条微博所对应的标签代码块列表 weibo_tags_list = soup.find_all('div', attrs=&#123;'class': 'c'&#125;, id=True) #微博发送时间与微博配图的字典 imgs_urls_dic = &#123;&#125; for entry, weibo_tag in enumerate(weibo_tags_list): print('正在爬取第%d页第%d条微博' % (page + 1, entry + 1)) #获取微博发送时间 if weibo_tag.find('span', text=re.compile('来自微博')): hms = ' '.join(weibo_tag.find( 'span', attrs=&#123;'class': 'ct'&#125;).text.replace('\xa0', ' ').replace(':', '-').split(' ')[:2]) else: hms = "none"+"-"+str(page)+"-"+str(entry) #该条微博若带有组图，获取组图中所有图片的URL if weibo_tag.find('a', text=re.compile('组图')): imgs_url = weibo_tag.find('a', text=re.compile('组图')).attrs['href'] html = requests.get(imgs_url, headers=headers) imgs_soup = bs(html.text, 'html.parser') imgs_tags_List = imgs_soup.find_all('img', alt='图片加载中...') img_urls_list = [imgs_tag.attrs['src'].replace( 'thumb180', 'large') for imgs_tag in imgs_tags_List] imgs_urls_dic[hms] = img_urls_list #该条微博仅有一张配图，或者没有图片，获取图片的URL else: img_tags_List = weibo_tag.find_all('img', alt='图片') img_urls_list = [img_tag.attrs['src'].replace( 'wap180', 'large') for img_tag in img_tags_List] imgs_urls_dic[hms] = img_urls_list print('第%d页微博爬取完毕,开始生成图片' % (page + 1)) for hms, img_urls_list in imgs_urls_dic.items(): for index, img_url in enumerate(img_urls_list): #生成图片的存放路径，图片被命名为微博发送时间 path = rootDir + hms #如果一条微博在同一时间发送了多张图片(即组图)的命名处理 if(index &gt; 0): path = path + '(' + str(index) + ')' path = path + '.jpg' try: if not os.path.exists(rootDir): #makedirs递归生成多级目录，mkdir仅能生成一级目录 os.makedirs(rootDir) if not os.path.exists(path): r = requests.get(img_url) with open(path, 'wb') as f: f.write(r.content) pictrue_num = pictrue_num + 1 print('success,成功爬取第%d张图片' % pictrue_num) else: print('%s已经存在' % path) except: print('爬取失败,%s' % img_url)print('总共爬取了%d张图片，存放在 %s 目录下' % (pictrue_num, rootDir))]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[纪念第一次用python]]></title>
    <url>%2F2019%2F03%2F18%2F%E7%BA%AA%E5%BF%B5%E7%AC%AC%E4%B8%80%E6%AC%A1%E7%94%A8python%2F</url>
    <content type="text"><![CDATA[用python3爬取百度贴吧图片1 寻找想要爬取图片的网页，如http://tieba.baidu.com/p/60273185722 查看网页源代码 3 放代码1234567891011121314151617181920212223242526272829303132333435363738# coding:utf-8import urllib.requestimport reimport osdef get_html(url): page = urllib.request.urlopen(url) html = page.read() return htmld = os.getcwd()p = "\\pic"isExists = os.path.exists(d+p)if not isExists: os.mkdir(d+p) print("创建文件夹成功")else: print("文件夹已存在") imgName = 0for i in range(3): html = get_html("http://tieba.baidu.com/p/6027318572?see_lz=1&amp;pn="+str(i+1)) html = html.decode('utf-8') reg = r'img class="BDE_Image" src="(.+?\.jpg)" size' reg_img = re.compile(reg) imglist = re.findall(reg_img, html) for imgPath in imglist: print(imgPath) f = open("pic/"+str(imgName)+".jpg", 'wb') f.write((urllib.request.urlopen(imgPath)).read()) f.close() imgName += 1print(str(imgName)+" img have been saved in pic")print("All Done!") 4 最终效果 附 半自动实现爬取图片，需要手输入链接及页数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# coding:utf-8import urllib.requestimport reimport timedef mkdir(): import os path = os.getcwd() LocalPath = input("Enter folder name to save pics(in now path): "); path = path+"\\"+LocalPath isExists=os.path.exists(path) if not isExists: # 如果不存在则创建目录 # 创建目录操作函数 os.makedirs(path) print (path+" 创建成功") else: # 如果目录存在则不创建，并提示目录已存在 print (path+" 已存在") return LocalPathLocalPath = mkdir()def get_html(url): page = urllib.request.urlopen(url) html = page.read() return htmldef get_url(): url = input("输入想要获取源码的网页链接: "); url = url+'?see_lz=' OrLz = input("只看楼主？(0否/1是): "); OrLz = str(OrLz) url = url+OrLz print(url) return urlurlFirst = get_url()page = input("输入页数:");page = int(page)imgName = 0for i in range(page): url = urlFirst+'&amp;pn='+str(i+1) print(url) html = get_html(url) html = html.decode('utf-8') reg = r'img class="BDE_Image" src="(.+?\.jpg)"' reg_img = re.compile(reg) imglist = re.findall(reg_img, html) for imgPath in imglist: #print(imgPath) f = open(LocalPath+"/"+str(imgName)+".jpg", 'wb') f.write((urllib.request.urlopen(imgPath)).read()) f.close() imgName += 1 if(imgName%10 == 0): print(str(imgName)+" imgs have been saved in "+LocalPath) time.sleep(5)print("All "+str(imgName)+" imgs have been saved in "+LocalPath) 问题: urllib.error.URLError: &lt;urlopen error [WinError 10060] 由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败。]]></content>
      <tags>
        <tag>Saito Asuka</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo上传图片]]></title>
    <url>%2F2019%2F03%2F10%2Fhexo%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[如何上传本地图片1 把主页配置文件_config.yml 里的post_asset_folder:这个选项设置为true2 在hexo目录下执行 npm install hexo-asset-image –save安装一个可以上传本地图片的插件3 等待一小段时间后，再运行hexo n “xxxx”来生成md博文时，/source/_posts文件夹内除了xxxx.md文件还会生成一个同名的文件夹4 最后在xxxx.md中想引入图片时，先把图片复制到xxxx这个文件夹中，然后只需要在xxxx.md中按照markdown的格式引入图片：![你想输入的替代文字](xxxx/图片名.jpg) 如何上传在线图片1 选择图床，如https://sm.ms/2 上传图片后就能直接生成带有链接的图片 代码为1[![鸟2.jpg](https://i.loli.net/2019/03/18/5c8f7ed81bffd.jpg)](https://i.loli.net/2019/03/18/5c8f7ed81bffd.jpg)]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>Saito Asuka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 使用]]></title>
    <url>%2F2019%2F03%2F10%2FHexo-%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[hexo new “postName” #新建文章hexo new page “pageName” #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，’ctrl + c’关闭server）hexo deploy #部署到GitHubhexo help # 查看帮助hexo version #查看Hexo的版本 删除文章直接本地删除md文件，然后执行hexo clean 使用时只需如 hexo g 一样写出首字母就行 hexo s -g #生成并本地预览hexo d -g #生成并上传 设置文章摘要的长度:在合适的位置加上&lt;!--more--&gt;即可]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Saito Asuka]]></title>
    <url>%2F2019%2F03%2F10%2FSaito-Asuka%2F</url>
    <content type="text"><![CDATA[齋藤飛鳥だいすき]]></content>
      <tags>
        <tag>Saito Asuka</tag>
      </tags>
  </entry>
</search>
